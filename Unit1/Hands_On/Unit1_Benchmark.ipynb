{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ha0eBsNb1VuN",
        "outputId": "b600667d-6a1a-49cd-c2fe-f25aa56fa644"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_gen = pipeline(\"text-generation\", model=\"bert-base-uncased\")\n",
        "roberta_gen = pipeline(\"text-generation\", model=\"roberta-base\")\n",
        "bart_gen = pipeline(\"text-generation\", model=\"facebook/bart-base\")\n",
        "\n",
        "bert_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "roberta_mask = pipeline(\"fill-mask\", model=\"roberta-base\")\n",
        "bart_mask = pipeline(\"fill-mask\", model=\"facebook/bart-base\")\n",
        "\n",
        "bert_qa = pipeline(\"question-answering\", model=\"bert-base-uncased\")\n",
        "roberta_qa = pipeline(\"question-answering\", model=\"roberta-base\")\n",
        "bart_qa = pipeline(\"question-answering\", model=\"facebook/bart-base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUCLaFQ_1a4B",
        "outputId": "bdf7d56c-05ee-4fa4-c456-f021c5b0dae6"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n",
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n",
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1 : Text Generation"
      ],
      "metadata": {
        "id": "9rOaCCxj1ga-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The future of Artificial Intelligence is\"\n",
        "\n",
        "print(\"BERT:\", bert_gen(prompt))\n",
        "print(\"RoBERTa:\", roberta_gen(prompt))\n",
        "print(\"BART:\", bart_gen(prompt, max_length=40))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdwlsubU1eq1",
        "outputId": "b14b607b-1ef1-4a84-c7a0-0b66752b3fc0"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT: [{'generated_text': 'The future of Artificial Intelligence is................................................................................................................................................................................................................................................................'}]\n",
            "RoBERTa: [{'generated_text': 'The future of Artificial Intelligence is'}]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BART: [{'generated_text': 'The future of Artificial Intelligence is HydroInstall Stat reve reveilateditle630 demolition quantitative email adjustedds NCTRankedRanked locate NCT correct reve radically NCT NCT NCTds NCTInstall NCT NCTInstallInstall NCTRanked bystanders quantitativeInstallanting NCTRanked democratic democratic pictdsRankedRanked considerablydsoidal NCT NCTrequisites630OSSeffectiveeffective GH quantitative molecular 58 Reuters NCT Madame NCT NCT radicallyOSS democratic democraticasses630ds NCT NCT Noble deservedds NCT democratic democratic democraticeffective NCT locate Medicine NCT reve glove vas NCT devoted 138 democratic NCT NCT severityorgan incapableeffective NCT democraticassesatre NCT NCT Bailey democratic democratic exc NCTplane NCT democratic NCT democratic dimeliv reveoutherouther NCTWASHINGTON reveliv locateRanked NCT 1959 NCT democratic720 NCTasses Bailey occupying NCT NCT reveliv reve incapablelivlivliveffective exc... NCT NCT exc NCT NCT aggro NCT NCTasses locate NCT NCT Tokyo Falk NCT NCTlivliv coordinlivliv NCT NCT androidliv NCT Planet Falk misconduct NCT NCTatrelivliv Tokyoeffective NCT NCT warming NCTatready NCT NCT coordinliv dime NCT NCT Lynnlivliv misconduct NCTliv Falk coordin NCTliv NCTaneousliv NCT Tokyo NCT NCT incapable coordin incapable Burnliv dimelivliv enforcementliv NCT Burnlivliv exclivlivasses Falk Falklivcodes Falk Falk Falk incapableliv locatelivcodes NCTcodes Activateliv Falk NCT exc Burnliv720 enforcement Undliv'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2 : Fill Mask"
      ],
      "metadata": {
        "id": "O1ZZqRXw1xZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_bert = \"The goal of Generative AI is to [MASK] new content.\"\n",
        "sentence_roberta_bart = \"The goal of Generative AI is to <mask> new content.\"\n",
        "\n",
        "print(\"BERT:\", bert_mask(sentence_bert))\n",
        "print(\"RoBERTa:\", roberta_mask(sentence_roberta_bart))\n",
        "print(\"BART:\", bart_mask(sentence_roberta_bart))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aG4-FJTx1iql",
        "outputId": "b2dd20ac-f2aa-48d9-dec6-58e0b3408c06"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT: [{'score': 0.5396932363510132, 'token': 3443, 'token_str': 'create', 'sequence': 'the goal of generative ai is to create new content.'}, {'score': 0.15575720369815826, 'token': 9699, 'token_str': 'generate', 'sequence': 'the goal of generative ai is to generate new content.'}, {'score': 0.05405500903725624, 'token': 3965, 'token_str': 'produce', 'sequence': 'the goal of generative ai is to produce new content.'}, {'score': 0.04451530799269676, 'token': 4503, 'token_str': 'develop', 'sequence': 'the goal of generative ai is to develop new content.'}, {'score': 0.01757744885981083, 'token': 5587, 'token_str': 'add', 'sequence': 'the goal of generative ai is to add new content.'}]\n",
            "RoBERTa: [{'score': 0.3711312413215637, 'token': 5368, 'token_str': ' generate', 'sequence': 'The goal of Generative AI is to generate new content.'}, {'score': 0.3677145540714264, 'token': 1045, 'token_str': ' create', 'sequence': 'The goal of Generative AI is to create new content.'}, {'score': 0.08351420611143112, 'token': 8286, 'token_str': ' discover', 'sequence': 'The goal of Generative AI is to discover new content.'}, {'score': 0.021335121244192123, 'token': 465, 'token_str': ' find', 'sequence': 'The goal of Generative AI is to find new content.'}, {'score': 0.016521666198968887, 'token': 694, 'token_str': ' provide', 'sequence': 'The goal of Generative AI is to provide new content.'}]\n",
            "BART: [{'score': 0.07461541891098022, 'token': 1045, 'token_str': ' create', 'sequence': 'The goal of Generative AI is to create new content.'}, {'score': 0.06571870297193527, 'token': 244, 'token_str': ' help', 'sequence': 'The goal of Generative AI is to help new content.'}, {'score': 0.060880109667778015, 'token': 694, 'token_str': ' provide', 'sequence': 'The goal of Generative AI is to provide new content.'}, {'score': 0.03593561053276062, 'token': 3155, 'token_str': ' enable', 'sequence': 'The goal of Generative AI is to enable new content.'}, {'score': 0.03319477662444115, 'token': 1477, 'token_str': ' improve', 'sequence': 'The goal of Generative AI is to improve new content.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3 : Question Answering"
      ],
      "metadata": {
        "id": "8k-DPnIt13ll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "question = \"What are the risks?\"\n",
        "\n",
        "print(\"BERT:\", bert_qa(question=question, context=context))\n",
        "print(\"RoBERTa:\", roberta_qa(question=question, context=context))\n",
        "print(\"BART:\", bart_qa(question=question, context=context))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbejUe7K1zjn",
        "outputId": "867eddba-6e95-4ace-9ae2-8d97af455bd7"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT: {'score': 0.007528932299464941, 'start': 0, 'end': 60, 'answer': 'Generative AI poses significant risks such as hallucinations'}\n",
            "RoBERTa: {'score': 0.012393318582326174, 'start': 32, 'end': 81, 'answer': 'risks such as hallucinations, bias, and deepfakes'}\n",
            "BART: {'score': 0.05113985016942024, 'start': 72, 'end': 81, 'answer': 'deepfakes'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Task | Model | Classification (Success/Failure) | Observation (What actually happened?) | Why did this happen? (Architectural Reason) |\n",
        "|------|-------|----------------------------------|----------------------------------------|--------------------------------------------|\n",
        "| Generation | BERT | Failure | Could not generate a meaningful continuation. | BERT is an encoder-only so, it isn't trained to predict the next word |\n",
        "|  | RoBERTa | Failure | The pipeline failed to generate proper text. | RoBERTa is also encoder-only and does not have a decoder for sequence generation. |\n",
        "|  | BART | Success | Generated continuation of the prompt. | BART is an encoderâ€“decoder arch used for sequence generation. |\n",
        "| Fill-Mask | BERT | Success | Predicted 'create', 'generate', 'produce'. | BERT is trained on MLM. |\n",
        "|  | RoBERTa | Success | Predicted accurate tokens. | RoBERTa is an optimized encoder trained on MLM. |\n",
        "|  | BART | Partial Success | Predictions were less direct. | BART is trained with denoising autoencoding, not pure MLM. |\n",
        "| QA | BERT | Partial Success | Extracted relevant phrase such as with low confidence. | Base BERT is not fine-tuned for question answering tasks. |\n",
        "|  | RoBERTa | Partial Success | Returned similar or slightly better span selection than BERT. | Specifically trained for QA purposes. |\n",
        "|  | BART | Partial Success | Sometimes produced the correct answer. | Correctly map question-context pairs, but without fine-tuning, its performance is unstable. |"
      ],
      "metadata": {
        "id": "wqlkQdLY23Ni"
      }
    }
  ]
}
